# run_all.py
import os
from generate_training_data_2 import create_training_dataset_from_enriched
from encode_attributes_3 import encode_attributes_and_save
from dataloader_4 import DamageDataset
from train_model_5 import train_model, MultimodalDamageClassifier
import pandas as pd
from torch.utils.data import DataLoader, random_split
import torch

# === ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹è¨­å®š ===
tier1_label_dir = "/mnt/bigdisk/xbd/geotiffs/tier1/labels"
tier3_label_dir = "/mnt/bigdisk/xbd/geotiffs/tier3/labels"
osm_root = "./output"
enriched_dir = "./enriched_all_buildings"
image_root = "/mnt/bigdisk/xbd/geotiffs"
raw_csv = "./training_dataset_raw.csv"
encoded_csv = "./training_dataset_with_labels_and_features_new.csv"
log_file = "train_report.txt"

# === ã‚¹ãƒ†ãƒƒãƒ— 1: xBD + OSM + å±æ€§è£œå®Œï¼ˆPart 1ï¼‰ ===
print("ğŸ› ï¸ Step 1: Preprocessing xBD + OSM buildings ...")
if not os.path.exists(enriched_dir):
    from preprocess_buildings_1 import process_all_disasters
    process_all_disasters(tier1_label_dir, tier3_label_dir, osm_root, enriched_dir)
print("âœ… Step 1 complete: enriched building CSVs generated.\n")

# === ã‚¹ãƒ†ãƒƒãƒ— 2: å­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿ä½œæˆï¼ˆPart 2ï¼‰ ===
print("ğŸ› ï¸ Step 2: Creating training dataset with pre_image path and labels ...")
create_training_dataset_from_enriched(enriched_dir, image_root, raw_csv)
print("âœ… Step 2 complete: training_dataset_raw.csv created.\n")

# === ã‚¹ãƒ†ãƒƒãƒ— 3: å±æ€§ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ & embeddingæ¬¡å…ƒèª¿æ•´ï¼ˆPart 3ï¼‰ ===
print("ğŸ› ï¸ Step 3: Encoding categorical attributes and computing embedding dims ...")
df, num_attr_classes, embedding_dims = encode_attributes_and_save(raw_csv, encoded_csv)
print("âœ… Step 3 complete: encoded CSV and embedding dims ready.\n")

# === Step 4: ç½å®³å˜ä½ã§ train/val ã‚’åˆ†å‰²ï¼ˆevent-level splitï¼‰ ===
print("ğŸ› ï¸ Step 4: Splitting data by disaster (event-level split) ...")
df = pd.read_csv(encoded_csv)
df["disaster"] = df["file"].apply(lambda x: x.split("_")[0])
all_disasters = sorted(df["disaster"].unique())
random.seed(42)
random.shuffle(all_disasters)
split_idx = int(0.8 * len(all_disasters))
train_disasters = all_disasters[:split_idx]
val_disasters = all_disasters[split_idx:]

train_df = df[df["disaster"].isin(train_disasters)].reset_index(drop=True)
val_df = df[df["disaster"].isin(val_disasters)].reset_index(drop=True)

train_df.to_csv("train_event_split.csv", index=False)
val_df.to_csv("val_event_split.csv", index=False)

print(f"âœ… Step 4 complete: {len(train_df)} train / {len(val_df)} val samples\n")

# === Step 5: å­¦ç¿’ãƒ»è©•ä¾¡ï¼ˆResNet + å±æ€§ï¼‰ ===
print("ğŸ› ï¸ Step 5: Training and evaluating the multimodal model ...")
from dataloader import DamageDataset

train_dataset = DamageDataset("train_event_split.csv")
val_dataset = DamageDataset("val_event_split.csv")

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = MultimodalDamageClassifier(num_attr_classes=num_attr_classes, embedding_dims=embedding_dims)
train_model(model, train_loader, val_loader, device, num_epochs=10, lr=1e-4, log_file=log_file)

print("ğŸ‰ All steps complete. Event-based evaluation is ready!")